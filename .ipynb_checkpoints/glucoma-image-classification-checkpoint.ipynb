{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5775c1",
   "metadata": {},
   "source": [
    "# Glaucoma Detection in Retinal Fundus Images\n",
    "\n",
    "The below code demonstrates the application of Convolutional Neural Network on the retinal images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddebe7c",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15e5143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: \n",
      " 2.2.1\n",
      "Keras ver:\n",
      " 2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "#import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow\n",
    "print(\"Tensorflow version: \\n\", tensorflow.__version__)\n",
    "import keras\n",
    "print(\"Keras ver:\\n\", keras.__version__)\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras import models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras import optimizers\n",
    "# Data Augmentation by introducing new modified random samples\n",
    "\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_curve\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "import cv2\n",
    "import os\n",
    "import shutil \n",
    "\n",
    "## versions--\n",
    "#pip install tensorflow==2.2.1\n",
    "#pip install keras==2.3.1\n",
    "#pip install opencv-python==4.4.0.40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f465dd",
   "metadata": {},
   "source": [
    "# Download Image Data from Google Drive Path \n",
    "\n",
    "Install the module `gdown` to download the image files (with .rar extension) to local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf6d1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.2; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown==4.2.0 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from gdown==4.2.0) (2.26.0)\n",
      "Requirement already satisfied: six in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from gdown==4.2.0) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from gdown==4.2.0) (4.8.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from gdown==4.2.0) (3.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from gdown==4.2.0) (4.62.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from beautifulsoup4->gdown==4.2.0) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from requests[socks]->gdown==4.2.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from requests[socks]->gdown==4.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from requests[socks]->gdown==4.2.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from requests[socks]->gdown==4.2.0) (2021.5.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from requests[socks]->gdown==4.2.0) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (from tqdm->gdown==4.2.0) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages\\urllib3\\connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages\\urllib3\\connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-08-8g-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fyF6UjU2y7Eit2-acbLfpFwJogP1QPR5\n",
      "To: C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNet\\collected_images.rar\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 22.5M/22.5M [01:08<00:00, 327kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'collected_images.rar'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org google-api-python-client==2.22.0\n",
    "#!pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org google-auth google-auth-oauthlib google-auth-httplib2\n",
    "!pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org gdown==4.2.0\n",
    "import gdown\n",
    "\n",
    "#https://drive.google.com/file/d/1fyF6UjU2y7Eit2-acbLfpFwJogP1QPR5/view?usp=sharing\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1fyF6UjU2y7Eit2-acbLfpFwJogP1QPR5'\n",
    "output = 'collected_images.rar'\n",
    "gdown.download(url, output, quiet=False, verify=False)\n",
    "\n",
    "#! gdown https://drive.google.com/uc?id=1fyF6UjU2y7Eit2-acbLfpFwJogP1QPR5 --no-verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed750ac",
   "metadata": {},
   "source": [
    "#  Extract Downloaded .rar files with `patoolib` Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af8260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNet\n",
      "Requirement already satisfied: patool in c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages (1.12)\n",
      "patool: Extracting collected_images.rar ...\n",
      "patool: running \"C:\\Program Files\\WinRAR\\rar.EXE\" x -- C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNet\\collected_images.rar\n",
      "patool:     with cwd=.\\Unpack_jki7cg44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.2; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: ... collected_images.rar extracted to `ImageClassification'.\n",
      "C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNet\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "!pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org patool\n",
    "import patoolib\n",
    "patoolib.extract_archive(\"collected_images.rar\")\n",
    "os.remove(\"collected_images.rar\") #remove .rar file because it is not required now\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89421a4",
   "metadata": {},
   "source": [
    "# Mount Image Data from Google Drive in Google Colab - (Not Required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd /content/drive/MyDrive/datasets/image-classification-dataset\n",
    "\n",
    "# #!pwd\n",
    "# !pip install unrar\n",
    "# !unrar x ImageClassification.rar\n",
    "# %cd ImageClassification\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef6c1b6",
   "metadata": {},
   "source": [
    "# Define Pathnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74323e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_pathname=os.getcwd()\n",
    "train_pathname= checkpoint_pathname + \"\\\\ImageClassification\\\\img_data\"\n",
    "test_pathname= checkpoint_pathname + \"\\\\ImageClassification\\\\test_imgs\"\n",
    "model_pathname=checkpoint_pathname\n",
    "\n",
    "train_dir = os.path.join(train_pathname, 'train')\n",
    "train_pos_ROP_dir = os.path.join(train_dir, 'train_pos_labels') # folder containing glucome eye images \n",
    "train_neg_ROP_dir = os.path.join(train_dir, 'train_neg_labels') # folder containing normal eye images[npn glucoma]\n",
    "\n",
    "\n",
    "test_dir = os.path.join(test_pathname, 'test_data')\n",
    "test_pos = os.path.join(test_dir, 'pos_label_images')  # folder containing glucoma test images\n",
    "test_pos_imgs = os.listdir(test_pos)\n",
    "test_neg = os.path.join(test_dir, 'nonpos_label_images') # folder containing normal eye images\n",
    "test_neg_imgs = os.listdir(test_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9238911",
   "metadata": {},
   "source": [
    "# Image Preprocessing and Define Custom Functions for Model Validation\n",
    "\n",
    "Image with the spot is labelled as a 1 and an image without any spot is labelled as 0. All the Images with and without spot are stored in the variable `temp_imgs` and the variable `temp_labels` has all the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c691fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the input image data:- (156, 150, 150, 3)\n",
      "\n",
      "Shape of the input image labels:- (156,)\n",
      "\n",
      "\n",
      "Input Array of Images:-\n",
      " [[[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]]\n",
      "\n",
      "Input Labels of Images:-\n",
      " [1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1\n",
      " 1 1 0 1 1 1 1 1]\n",
      "\n",
      "Number of samples in the input image tensor[4D] of color images:\n",
      " 156\n",
      "\n",
      " Label counts:\n",
      " 156\n"
     ]
    }
   ],
   "source": [
    "pos_ROP_imgs = os.listdir(train_pos_ROP_dir) ###\n",
    "\n",
    "y_1 = [] \n",
    "for i in pos_ROP_imgs:\n",
    "    y_1.append(1)\n",
    "     \n",
    "neg_ROP_imgs = os.listdir(train_neg_ROP_dir)  ###\n",
    "\n",
    "y_0 = [] \n",
    "for k in neg_ROP_imgs:\n",
    "    y_0.append(0)\n",
    "\n",
    "\n",
    "temp_imgs = pos_ROP_imgs + neg_ROP_imgs\n",
    "temp_labels =y_1 + y_0\n",
    "\n",
    "images_list = list(zip(temp_imgs, temp_labels)) \n",
    "random.seed(7)\n",
    "random.shuffle(images_list)\n",
    "images_list  #map label to each image (label 1 and label 0 for pos or neg image respectively and shuffle them randomly)\n",
    "temp_X = [i[0] for i in images_list]\n",
    "y = [i[1] for i in images_list]  # y labels\n",
    "\n",
    "\n",
    "def read_process_imgs(shuffled_imgs, neg_ROP_imgs, pos_ROP_imgs, neg_img_dir, pos_img_dir):\n",
    "    \n",
    "    X1 = []\n",
    "    #ref_imgfile = [] \n",
    "    for img in shuffled_imgs:\n",
    "        if img in neg_ROP_imgs:\n",
    "            filename = neg_img_dir + '/' + img\n",
    "            img1 = cv2.imread(filename)\n",
    "            #gaus_blur = cv2.GaussianBlur(img1, (11, 11), 0)\n",
    "            \n",
    "            X1.append(cv2.resize(img1, (150,150)))  ### image resized to (150,150,3)  from (480, 640, 3)\n",
    "            #large the size of resized image, better could be the accuracy but on the other side, training time increase\n",
    "            #ref_imgfile.append(img)\n",
    "        \n",
    "        if img in pos_ROP_imgs:\n",
    "            filename = pos_img_dir + '/' + img\n",
    "            img1 = cv2.imread(filename)\n",
    "            #gaus_blur = cv2.GaussianBlur(img1, (11, 11), 0)\n",
    "            #print(cv2.resize(img1, (150,150)).shape)\n",
    "            X1.append(cv2.resize(img1, (150,150)))\n",
    "            #ref_imgfile.append(img)\n",
    "    \n",
    "    return X1 #ref_imgfile\n",
    "\n",
    "X = read_process_imgs(temp_X, neg_ROP_imgs, pos_ROP_imgs, train_neg_ROP_dir, train_pos_ROP_dir) \n",
    "\n",
    "#X[1]\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "inp_X = np.array(X)\n",
    "print(\"\\nShape of the input image data:-\", inp_X.shape)\n",
    "inp_y = np.array(y)\n",
    "print(\"\\nShape of the input image labels:-\", inp_y.shape)\n",
    "\n",
    "print(\"\\n\\nInput Array of Images:-\\n\", inp_X[0:1, :, :])\n",
    "print(\"\\nInput Labels of Images:-\\n\", inp_y)\n",
    "\n",
    "print(\"\\nNumber of samples in the input image tensor[4D] of color images:\\n\", len(inp_X))\n",
    "print(\"\\n Label counts:\\n\", len(inp_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc29992",
   "metadata": {},
   "source": [
    "# Model Compilation Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f98384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model build or compile - add convolutions\n",
    "model = models.Sequential() #sequential model\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3))) # pass input arguments to Conv2D\n",
    "model.add(layers.MaxPool2D(2, 2))  #down sampling the features\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))  #relu to ensure that we don't have negative pixels\n",
    "model.add(layers.MaxPool2D(2, 2))   #\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPool2D(2, 2))\n",
    "\n",
    "model.add(layers.Flatten()) # flattening the conv output\n",
    "\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid')) # binary output\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
    "              metrics=['acc', 'mse', recall_m, precision_m, f1_m]) # by default metrics has accuracy and mse functions. \n",
    "# we have added recall, precision, and f1 score metrics as well \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1f330",
   "metadata": {},
   "source": [
    "# Model Training and Saving the Best Model\n",
    "\n",
    "ImageDataGenerator accepts the original data, randomly transforms it, and returns only the new, transformed data.\n",
    "Now the input data is slightly modified versions of the original input data, the network is able to learn more robust features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ee90f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 3s - loss: 0.5176 - acc: 0.7628 - mse: 0.1502 - recall_m: 0.7970 - precision_m: 0.7970 - f1_m: 0.7609\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.51758, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 2/30\n",
      " - 3s - loss: 0.3972 - acc: 0.7885 - mse: 0.1411 - recall_m: 0.8269 - precision_m: 0.8697 - f1_m: 0.8209\n",
      "\n",
      "Epoch 00002: loss improved from 0.51758 to 0.39717, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 3/30\n",
      " - 3s - loss: 0.5669 - acc: 0.8141 - mse: 0.1474 - recall_m: 0.8120 - precision_m: 0.8718 - f1_m: 0.8028\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.39717\n",
      "Epoch 4/30\n",
      " - 3s - loss: 0.3325 - acc: 0.8526 - mse: 0.1108 - recall_m: 0.9038 - precision_m: 0.9145 - f1_m: 0.8834\n",
      "\n",
      "Epoch 00004: loss improved from 0.39717 to 0.33247, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 5/30\n",
      " - 3s - loss: 0.4970 - acc: 0.8013 - mse: 0.1290 - recall_m: 0.8376 - precision_m: 0.8910 - f1_m: 0.8330\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.33247\n",
      "Epoch 6/30\n",
      " - 3s - loss: 0.4061 - acc: 0.8077 - mse: 0.1276 - recall_m: 0.7970 - precision_m: 0.8440 - f1_m: 0.7973\n",
      "\n",
      "Epoch 00006: loss did not improve from 0.33247\n",
      "Epoch 7/30\n",
      " - 3s - loss: 0.3396 - acc: 0.8526 - mse: 0.1086 - recall_m: 0.8269 - precision_m: 0.9466 - f1_m: 0.8630\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.33247\n",
      "Epoch 8/30\n",
      " - 3s - loss: 0.4100 - acc: 0.8333 - mse: 0.1244 - recall_m: 0.7949 - precision_m: 0.8333 - f1_m: 0.7949\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.33247\n",
      "Epoch 9/30\n",
      " - 3s - loss: 0.3420 - acc: 0.8333 - mse: 0.1130 - recall_m: 0.8333 - precision_m: 0.8547 - f1_m: 0.8236\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.33247\n",
      "Epoch 10/30\n",
      " - 3s - loss: 0.3392 - acc: 0.8077 - mse: 0.1180 - recall_m: 0.8056 - precision_m: 0.8996 - f1_m: 0.8238\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.33247\n",
      "Epoch 11/30\n",
      " - 3s - loss: 0.3068 - acc: 0.8654 - mse: 0.1013 - recall_m: 0.8697 - precision_m: 0.9380 - f1_m: 0.8716\n",
      "\n",
      "Epoch 00011: loss improved from 0.33247 to 0.30684, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 12/30\n",
      " - 3s - loss: 0.2889 - acc: 0.8654 - mse: 0.0965 - recall_m: 0.8697 - precision_m: 0.9124 - f1_m: 0.8657\n",
      "\n",
      "Epoch 00012: loss improved from 0.30684 to 0.28887, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 13/30\n",
      " - 3s - loss: 0.3178 - acc: 0.8654 - mse: 0.0937 - recall_m: 0.8675 - precision_m: 0.9081 - f1_m: 0.8689\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.28887\n",
      "Epoch 14/30\n",
      " - 3s - loss: 0.3110 - acc: 0.8397 - mse: 0.0993 - recall_m: 0.8825 - precision_m: 0.9124 - f1_m: 0.8720\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.28887\n",
      "Epoch 15/30\n",
      " - 3s - loss: 0.2520 - acc: 0.8782 - mse: 0.0861 - recall_m: 0.8846 - precision_m: 0.9060 - f1_m: 0.8730\n",
      "\n",
      "Epoch 00015: loss improved from 0.28887 to 0.25195, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 16/30\n",
      " - 3s - loss: 0.3270 - acc: 0.8846 - mse: 0.0911 - recall_m: 0.9060 - precision_m: 0.8953 - f1_m: 0.8817\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.25195\n",
      "Epoch 17/30\n",
      " - 3s - loss: 0.2393 - acc: 0.9038 - mse: 0.0741 - recall_m: 0.8953 - precision_m: 0.9252 - f1_m: 0.8901\n",
      "\n",
      "Epoch 00017: loss improved from 0.25195 to 0.23928, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 18/30\n",
      " - 3s - loss: 0.2653 - acc: 0.9295 - mse: 0.0591 - recall_m: 0.9359 - precision_m: 0.9274 - f1_m: 0.9212\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.23928\n",
      "Epoch 19/30\n",
      " - 3s - loss: 0.2253 - acc: 0.8782 - mse: 0.0727 - recall_m: 0.8932 - precision_m: 0.8718 - f1_m: 0.8674\n",
      "\n",
      "Epoch 00019: loss improved from 0.23928 to 0.22531, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 20/30\n",
      " - 3s - loss: 0.2493 - acc: 0.8974 - mse: 0.0717 - recall_m: 0.9103 - precision_m: 0.9188 - f1_m: 0.9011\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.22531\n",
      "Epoch 21/30\n",
      " - 3s - loss: 0.1879 - acc: 0.8846 - mse: 0.0651 - recall_m: 0.9338 - precision_m: 0.9274 - f1_m: 0.9142\n",
      "\n",
      "Epoch 00021: loss improved from 0.22531 to 0.18791, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 22/30\n",
      " - 3s - loss: 0.1524 - acc: 0.9615 - mse: 0.0416 - recall_m: 0.9594 - precision_m: 0.9786 - f1_m: 0.9604\n",
      "\n",
      "Epoch 00022: loss improved from 0.18791 to 0.15236, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 23/30\n",
      " - 3s - loss: 0.2425 - acc: 0.8910 - mse: 0.0713 - recall_m: 0.9316 - precision_m: 0.9274 - f1_m: 0.9118\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.15236\n",
      "Epoch 24/30\n",
      " - 3s - loss: 0.1512 - acc: 0.9359 - mse: 0.0467 - recall_m: 0.9530 - precision_m: 0.9658 - f1_m: 0.9521\n",
      "\n",
      "Epoch 00024: loss improved from 0.15236 to 0.15119, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 25/30\n",
      " - 3s - loss: 0.2342 - acc: 0.9423 - mse: 0.0424 - recall_m: 0.9658 - precision_m: 0.9615 - f1_m: 0.9558\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.15119\n",
      "Epoch 26/30\n",
      " - 3s - loss: 0.0937 - acc: 0.9615 - mse: 0.0279 - recall_m: 0.9487 - precision_m: 0.9487 - f1_m: 0.9431\n",
      "\n",
      "Epoch 00026: loss improved from 0.15119 to 0.09366, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 27/30\n",
      " - 3s - loss: 0.1583 - acc: 0.9295 - mse: 0.0537 - recall_m: 0.9466 - precision_m: 0.9038 - f1_m: 0.9123\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.09366\n",
      "Epoch 28/30\n",
      " - 3s - loss: 0.0853 - acc: 0.9679 - mse: 0.0265 - recall_m: 0.9744 - precision_m: 0.9786 - f1_m: 0.9709\n",
      "\n",
      "Epoch 00028: loss improved from 0.09366 to 0.08530, saving model to C:\\Users\\AG89382\\AppData\\Local\\Programs\\Python\\Python37\\deepenv\\glucoma-image-classification\\glucoma-image-classification-ConvNetrop_model.hdf5\n",
      "Epoch 29/30\n",
      " - 3s - loss: 0.3299 - acc: 0.9295 - mse: 0.0607 - recall_m: 0.9081 - precision_m: 0.9423 - f1_m: 0.9123\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.08530\n",
      "Epoch 30/30\n",
      " - 3s - loss: 0.1012 - acc: 0.9551 - mse: 0.0321 - recall_m: 0.9274 - precision_m: 0.9231 - f1_m: 0.9192\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.08530\n"
     ]
    }
   ],
   "source": [
    "#model training- checkpointed model to ensure that best model is saved during run having minimum comparative loss\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "ntrain = len(inp_X)  #xTrain\n",
    "batch_size = 4\n",
    "train_generator = train_datagen.flow(inp_X, inp_y, batch_size = batch_size)\n",
    "\n",
    "'save model if there is improvement in loss'\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_pathname + \"rop_model.hdf5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto') # period = number of epochs after which keeping save the model.\n",
    "#give the model name by which it is to be stored \n",
    "\n",
    "\n",
    "\n",
    "history_callback = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=ntrain // batch_size,  \n",
    "      epochs=30,\n",
    "      #validation_data=validation_generator,\n",
    "      #validation_steps=5,  \n",
    "      verbose=2, callbacks=[checkpoint])  # True while running in server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145c78c",
   "metadata": {},
   "source": [
    "# Trace Loss (while model training) and Save Logs\n",
    "\n",
    "We observe that the loss has consistently decreased and was 0.211369 in final epoch. \n",
    "Accuracy improved as loss decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d125ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>mse</th>\n",
       "      <th>recall_m</th>\n",
       "      <th>precision_m</th>\n",
       "      <th>f1_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.517579</td>\n",
       "      <td>0.762821</td>\n",
       "      <td>0.150151</td>\n",
       "      <td>0.797008</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.760928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.397172</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.141092</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.869658</td>\n",
       "      <td>0.820879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.566922</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>0.147357</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.802808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.332472</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.110753</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.883394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.497046</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.129045</td>\n",
       "      <td>0.837607</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.832967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.406084</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.127619</td>\n",
       "      <td>0.797008</td>\n",
       "      <td>0.844017</td>\n",
       "      <td>0.797314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.339636</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.108646</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.946581</td>\n",
       "      <td>0.863004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.410005</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.124412</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.794872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.341998</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.112954</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.854701</td>\n",
       "      <td>0.823565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.339205</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.118018</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.899572</td>\n",
       "      <td>0.823809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.306838</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.101331</td>\n",
       "      <td>0.869658</td>\n",
       "      <td>0.938034</td>\n",
       "      <td>0.871551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.288871</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.096498</td>\n",
       "      <td>0.869658</td>\n",
       "      <td>0.912393</td>\n",
       "      <td>0.865690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.317805</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.093686</td>\n",
       "      <td>0.867521</td>\n",
       "      <td>0.908120</td>\n",
       "      <td>0.868864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.311016</td>\n",
       "      <td>0.839744</td>\n",
       "      <td>0.099343</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.912393</td>\n",
       "      <td>0.872039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.251951</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.086063</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>0.873016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.091095</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>0.895299</td>\n",
       "      <td>0.881685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.239282</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.074051</td>\n",
       "      <td>0.895299</td>\n",
       "      <td>0.925214</td>\n",
       "      <td>0.890110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.265347</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.059145</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.921245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.225315</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.072738</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.867399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.249319</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.071744</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.918804</td>\n",
       "      <td>0.901099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.187909</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.065091</td>\n",
       "      <td>0.933761</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.914164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.152357</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.041644</td>\n",
       "      <td>0.959402</td>\n",
       "      <td>0.978633</td>\n",
       "      <td>0.960440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.242511</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.071267</td>\n",
       "      <td>0.931624</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.911844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.151190</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.046686</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.952137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.234162</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.093665</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.027861</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.943101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.158297</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.946581</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.912332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.085302</td>\n",
       "      <td>0.967949</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.978633</td>\n",
       "      <td>0.970940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.329912</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.060662</td>\n",
       "      <td>0.908120</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.912332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.101185</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.032133</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.919170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       acc       mse  recall_m  precision_m      f1_m\n",
       "0   0.517579  0.762821  0.150151  0.797008     0.797009  0.760928\n",
       "1   0.397172  0.788462  0.141092  0.826923     0.869658  0.820879\n",
       "2   0.566922  0.814103  0.147357  0.811966     0.871795  0.802808\n",
       "3   0.332472  0.852564  0.110753  0.903846     0.914530  0.883394\n",
       "4   0.497046  0.801282  0.129045  0.837607     0.891026  0.832967\n",
       "5   0.406084  0.807692  0.127619  0.797008     0.844017  0.797314\n",
       "6   0.339636  0.852564  0.108646  0.826923     0.946581  0.863004\n",
       "7   0.410005  0.833333  0.124412  0.794872     0.833333  0.794872\n",
       "8   0.341998  0.833333  0.112954  0.833333     0.854701  0.823565\n",
       "9   0.339205  0.807692  0.118018  0.805556     0.899572  0.823809\n",
       "10  0.306838  0.865385  0.101331  0.869658     0.938034  0.871551\n",
       "11  0.288871  0.865385  0.096498  0.869658     0.912393  0.865690\n",
       "12  0.317805  0.865385  0.093686  0.867521     0.908120  0.868864\n",
       "13  0.311016  0.839744  0.099343  0.882479     0.912393  0.872039\n",
       "14  0.251951  0.878205  0.086063  0.884615     0.905983  0.873016\n",
       "15  0.327000  0.884615  0.091095  0.905983     0.895299  0.881685\n",
       "16  0.239282  0.903846  0.074051  0.895299     0.925214  0.890110\n",
       "17  0.265347  0.929487  0.059145  0.935897     0.927350  0.921245\n",
       "18  0.225315  0.878205  0.072738  0.893162     0.871795  0.867399\n",
       "19  0.249319  0.897436  0.071744  0.910256     0.918804  0.901099\n",
       "20  0.187909  0.884615  0.065091  0.933761     0.927350  0.914164\n",
       "21  0.152357  0.961538  0.041644  0.959402     0.978633  0.960440\n",
       "22  0.242511  0.891026  0.071267  0.931624     0.927350  0.911844\n",
       "23  0.151190  0.935897  0.046686  0.952991     0.965812  0.952137\n",
       "24  0.234162  0.942308  0.042423  0.965812     0.961538  0.955800\n",
       "25  0.093665  0.961538  0.027861  0.948718     0.948718  0.943101\n",
       "26  0.158297  0.929487  0.053711  0.946581     0.903846  0.912332\n",
       "27  0.085302  0.967949  0.026505  0.974359     0.978633  0.970940\n",
       "28  0.329912  0.929487  0.060662  0.908120     0.942308  0.912332\n",
       "29  0.101185  0.955128  0.032133  0.927350     0.923077  0.919170"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss_history = history_callback.history\n",
    "\n",
    "lossdata = pd.DataFrame.from_dict(loss_history)\n",
    "\n",
    "lossdata.to_csv(checkpoint_pathname + 'losshistory.csv')\n",
    "lossdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c148b",
   "metadata": {},
   "source": [
    "# Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb1b9a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "dependencies = {\n",
    "     'recall_m': recall_m, \n",
    "     'precision_m': precision_m,\n",
    "     'f1_m': f1_m,\n",
    "}\n",
    "\n",
    "model1 = load_model(model_pathname + 'rop_model.hdf5', custom_objects=dependencies) # load the trained model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f3c76",
   "metadata": {},
   "source": [
    "#  Predict on the Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5398db5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of Test Images:\n",
      " 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ag89382\\appdata\\local\\programs\\python\\python37\\deepenv\\lib\\site-packages\\ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mod_performance:\n",
      "     fpr       tpr    thresholds\n",
      "0   0.0  0.000000  1.374547e+00\n",
      "1   0.1  0.000000  3.745471e-01\n",
      "2   0.1  0.071429  4.182339e-03\n",
      "3   0.3  0.071429  3.673941e-03\n",
      "4   0.3  0.214286  1.859374e-05\n",
      "5   0.4  0.214286  3.294289e-07\n",
      "6   0.4  0.571429  2.436468e-10\n",
      "7   0.5  0.571429  1.524025e-10\n",
      "8   0.5  0.714286  1.153487e-12\n",
      "9   0.6  0.714286  9.756131e-13\n",
      "10  0.6  1.000000  9.673154e-14\n",
      "11  1.0  1.000000  1.169910e-20 \n",
      "\n",
      " Confusion matrix: \n",
      " [[10 14]\n",
      " [ 0  0]] \n",
      "\n",
      " Recall is: nan Precision is: 0.00 Accuracy is: 0.42\n"
     ]
    }
   ],
   "source": [
    "#model validation on test data -\n",
    "#Image preprocessing on test images\n",
    "print(\"\\nLength of Test Images:\\n\", len(test_pos_imgs) + len(test_neg_imgs))\n",
    "\n",
    "test_y_1 = [] \n",
    "for i in test_pos_imgs:\n",
    "    test_y_1.append(1)\n",
    "\n",
    "  ###\n",
    "\n",
    "test_y_0 = [] \n",
    "for k in test_neg_imgs:\n",
    "    test_y_0.append(0)\n",
    "    \n",
    "test_imgs = test_pos_imgs + test_neg_imgs\n",
    "test_labels = test_y_1 + test_y_0\n",
    "\n",
    "def transform_test_imgs(filename):\n",
    "    \n",
    "    if filename in test_pos_imgs:\n",
    "        test_img = cv2.imread(test_pos + '/' + filename) #test_pos_imgs[0]\n",
    "        #test_img1 = cv2.GaussianBlur(test_img, (11, 11), 0)  # gauss_blur\n",
    "        test_img2 = cv2.resize(test_img , (150,150))   #resize\n",
    "        \n",
    "    else:\n",
    "        test_img = cv2.imread(test_neg + '/' + filename) #test_pos_imgs[0]\n",
    "        #test_img1 = cv2.GaussianBlur(test_img, (11, 11), 0)  # gauss_blur\n",
    "        test_img2 = cv2.resize(test_img , (150,150))   #resize\n",
    "    \n",
    "    return test_img2\n",
    "\n",
    "\n",
    "test_img_list = []\n",
    "for f in test_imgs:\n",
    "    test_X = transform_test_imgs(f)  #images in test_x is in same order as we have test_labels.\n",
    "    test_img_list.append(test_X)\n",
    "    \n",
    "    \n",
    "test_arr = np.array(test_img_list)\n",
    "test_arr1 = test_arr.astype('float32')\n",
    "test_arr2 = test_arr1 / 255.\n",
    "\n",
    "\n",
    "#check precision, recall, accuracy on test images\n",
    "len(test_arr2)\n",
    "#predictedlabels=model1.predict_classes(test_arr2)\n",
    "predicted_probs = model1.predict(test_arr2)\n",
    "predictedlabels=np.argmax(predicted_probs,axis=1)\n",
    "\n",
    "conf_mat = confusion_matrix(predictedlabels, test_labels)  \n",
    "Recall = conf_mat[1,1]/(conf_mat[1,1]+conf_mat[1,0])\n",
    "Precision = conf_mat[1,1]/(conf_mat[1,1]+conf_mat[0,1])\n",
    "Accuracy = (conf_mat[1,1]+conf_mat[0,0])/(conf_mat[0,0]+conf_mat[0,1]+conf_mat[1,0]+conf_mat[1,1])\n",
    "\n",
    "# RoC\n",
    "y_pred = model1.predict(test_arr2).ravel()  # flatten the array of predicted values.\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, y_pred)\n",
    "auc_keras = auc(fpr, tpr)  # 90.28 \n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "perf_metrics= list(zip(fpr, tpr, thresholds)) \n",
    "mod_perf = pd.DataFrame(perf_metrics, columns = ['fpr', 'tpr', 'thresholds'])\n",
    "print(\"\\nmod_performance:\\n {} \\n\\n Confusion matrix: \\n {} \\n\\n Recall is: {:.2f} Precision is: {:.2f} Accuracy is: {:.2f}\"\n",
    "      .format(mod_perf, conf_mat, Recall, Precision, Accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db646d6b",
   "metadata": {},
   "source": [
    "Accuracy in the final epoch on the train set is approximately 96%\n",
    "and the accuracy on the test set is approximately 75%. \n",
    "\n",
    "It is a case of model overfitting that can be attributed to the imbalanced dataset as well as the small sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c090881",
   "metadata": {},
   "source": [
    "# Delete the Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6499acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally remove the ImageClassification directory. We can reload it in further runs.\n",
    "shutil.rmtree(\"ImageClassification\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##Convolution neural networks-\n",
    "#https://arxiv.org/ftp/arxiv/papers/1506/1506.01195.pdf\n",
    "#https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "\n",
    "# ImageDataGenerator accepts the original data, randomly transforms it, and returns only the new, transformed data.\n",
    "#Input data is slightly modified versions of the original input data, the network is able to learn more robust features.\n",
    "\n",
    "\n",
    "##Data Augmentation-\n",
    "#https://arxiv.org/abs/2004.13529  ; augmentation behavioural cloning to generate data.\n",
    "#file:///C:/Users/AG89382/Downloads/data-06-00014.pdf\n",
    "\n",
    "##Input Dataset-\n",
    "#https://ieee-dataport.org/documents/1450-fundus-images-899-glaucoma-data-and-551-normal-data\n",
    "\n",
    "##Other referred articles-\n",
    "#https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0263-7\n",
    "#https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
