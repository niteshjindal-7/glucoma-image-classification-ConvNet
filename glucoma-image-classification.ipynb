{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5775c1",
   "metadata": {},
   "source": [
    "# Glaucoma Detection in Retinal Fundus Images\n",
    "\n",
    "The below exercise demonstrates the application of Convolutional Neural Network on the retinal images. \n",
    "\n",
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15e5143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: \n",
      " 2.2.1\n",
      "Keras ver:\n",
      " 2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "#import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow\n",
    "print(\"Tensorflow version: \\n\", tensorflow.__version__)\n",
    "import keras\n",
    "print(\"Keras ver:\\n\", keras.__version__)\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras import models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras import optimizers\n",
    "# Data Augmentation by introducing new modified random samples\n",
    "\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_curve\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "## versions--\n",
    "#pip install tensorflow==2.2.1\n",
    "#pip install keras==2.3.1\n",
    "#pip install opencv-python==4.4.0.40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89421a4",
   "metadata": {},
   "source": [
    "# Mount Image Data from Google Drive - (Executeble in Google Colab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58e93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/MyDrive/datasets/image-classification-dataset\n",
    "\n",
    "#!pwd\n",
    "!pip install unrar\n",
    "!unrar x ImageClassification.rar\n",
    "%cd ImageClassification\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee53f68",
   "metadata": {},
   "source": [
    "# Define Pathnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74323e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_pathname=os.getcwd()\n",
    "train_pathname= checkpoint_pathname + \"/img_data\"\n",
    "test_pathname= checkpoint_pathname + \"/test_imgs\"\n",
    "model_pathname=checkpoint_pathname\n",
    "\n",
    "train_dir = os.path.join(train_pathname, 'train')\n",
    "train_pos_ROP_dir = os.path.join(train_dir, 'train_pos_labels') # folder containing glucome eye images \n",
    "train_neg_ROP_dir = os.path.join(train_dir, 'train_neg_labels') # folder containing normal eye images[npn glucoma]\n",
    "\n",
    "\n",
    "test_dir = os.path.join(test_pathname, 'test_data')\n",
    "test_pos = os.path.join(test_dir, 'pos_label_images')  # folder containing glucoma test images\n",
    "test_pos_imgs = os.listdir(test_pos)\n",
    "test_neg = os.path.join(test_dir, 'nonpos_label_images') # folder containing normal eye images\n",
    "test_neg_imgs = os.listdir(test_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9238911",
   "metadata": {},
   "source": [
    "# Image Preprocessing and Define Custom Functions for Model Validation\n",
    "\n",
    "Image with the spot is labelled as a 1 and an image without any spot is labelled as 0. All the Images with and without spot are stored in the variable `temp_imgs` and the variable `temp_labels` has all the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c691fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the input image data:- (156, 150, 150, 3)\n",
      "\n",
      "Shape of the input image labels:- (156,)\n",
      "\n",
      "\n",
      "Input Array of Images:-\n",
      " [[[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]]\n",
      "\n",
      "Input Labels of Images:-\n",
      " [1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1\n",
      " 1 1 0 1 1 1 1 1]\n",
      "\n",
      "Number of samples in the input image tensor[4D] of color images:\n",
      " 156\n",
      "\n",
      " Label counts:\n",
      " 156\n"
     ]
    }
   ],
   "source": [
    "pos_ROP_imgs = os.listdir(train_pos_ROP_dir) ###\n",
    "\n",
    "y_1 = [] \n",
    "for i in pos_ROP_imgs:\n",
    "    y_1.append(1)\n",
    "     \n",
    "neg_ROP_imgs = os.listdir(train_neg_ROP_dir)  ###\n",
    "\n",
    "y_0 = [] \n",
    "for k in neg_ROP_imgs:\n",
    "    y_0.append(0)\n",
    "\n",
    "\n",
    "temp_imgs = pos_ROP_imgs + neg_ROP_imgs\n",
    "temp_labels =y_1 + y_0\n",
    "\n",
    "images_list = list(zip(temp_imgs, temp_labels)) \n",
    "random.seed(7)\n",
    "random.shuffle(images_list)\n",
    "images_list  #map label to each image (label 1 and label 0 for pos or neg image respectively and shuffle them randomly)\n",
    "temp_X = [i[0] for i in images_list]\n",
    "y = [i[1] for i in images_list]  # y labels\n",
    "\n",
    "\n",
    "def read_process_imgs(shuffled_imgs, neg_ROP_imgs, pos_ROP_imgs, neg_img_dir, pos_img_dir):\n",
    "    \n",
    "    X1 = []\n",
    "    #ref_imgfile = [] \n",
    "    for img in shuffled_imgs:\n",
    "        if img in neg_ROP_imgs:\n",
    "            filename = neg_img_dir + '/' + img\n",
    "            img1 = cv2.imread(filename)\n",
    "            #gaus_blur = cv2.GaussianBlur(img1, (11, 11), 0)\n",
    "            \n",
    "            X1.append(cv2.resize(img1, (150,150)))  ### image resized to (150,150,3)  from (480, 640, 3)\n",
    "            #large the size of resized image, better could be the accuracy but on the other side, training time increase\n",
    "            #ref_imgfile.append(img)\n",
    "        \n",
    "        if img in pos_ROP_imgs:\n",
    "            filename = pos_img_dir + '/' + img\n",
    "            img1 = cv2.imread(filename)\n",
    "            #gaus_blur = cv2.GaussianBlur(img1, (11, 11), 0)\n",
    "            #print(cv2.resize(img1, (150,150)).shape)\n",
    "            X1.append(cv2.resize(img1, (150,150)))\n",
    "            #ref_imgfile.append(img)\n",
    "    \n",
    "    return X1 #ref_imgfile\n",
    "\n",
    "X = read_process_imgs(temp_X, neg_ROP_imgs, pos_ROP_imgs, train_neg_ROP_dir, train_pos_ROP_dir) \n",
    "\n",
    "#X[1]\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "inp_X = np.array(X)\n",
    "print(\"\\nShape of the input image data:-\", inp_X.shape)\n",
    "inp_y = np.array(y)\n",
    "print(\"\\nShape of the input image labels:-\", inp_y.shape)\n",
    "\n",
    "print(\"\\n\\nInput Array of Images:-\\n\", inp_X[0:1, :, :])\n",
    "print(\"\\nInput Labels of Images:-\\n\", inp_y)\n",
    "\n",
    "print(\"\\nNumber of samples in the input image tensor[4D] of color images:\\n\", len(inp_X))\n",
    "print(\"\\n Label counts:\\n\", len(inp_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc29992",
   "metadata": {},
   "source": [
    "# Model Compilation Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f98384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model build or compile - add convolutions\n",
    "model = models.Sequential() #sequential model\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3))) # pass input arguments to Conv2D\n",
    "model.add(layers.MaxPool2D(2, 2))  #down sampling the features\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))  #relu to ensure that we don't have negative pixels\n",
    "model.add(layers.MaxPool2D(2, 2))   #\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPool2D(2, 2))\n",
    "\n",
    "model.add(layers.Flatten()) # flattening the conv output\n",
    "\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid')) # binary output\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
    "              metrics=['acc', 'mse', recall_m, precision_m, f1_m]) # by default metrics has accuracy and mse functions. \n",
    "# we have added recall, precision, and f1 score metrics as well \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1f330",
   "metadata": {},
   "source": [
    "# Model Training and Saving the Best Model\n",
    "\n",
    "ImageDataGenerator accepts the original data, randomly transforms it, and returns only the new, transformed data.\n",
    "Now the input data is slightly modified versions of the original input data, the network is able to learn more robust features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ee90f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 3s - loss: 0.6752 - acc: 0.7436 - mse: 0.1637 - recall_m: 0.8312 - precision_m: 0.8483 - f1_m: 0.7969\n",
      "Epoch 2/30\n",
      " - 3s - loss: 0.4741 - acc: 0.7756 - mse: 0.1530 - recall_m: 0.7756 - precision_m: 0.9081 - f1_m: 0.8016\n",
      "Epoch 3/30\n",
      " - 3s - loss: 0.3497 - acc: 0.8333 - mse: 0.1159 - recall_m: 0.8269 - precision_m: 0.8910 - f1_m: 0.8374\n",
      "Epoch 4/30\n",
      " - 3s - loss: 0.3990 - acc: 0.7821 - mse: 0.1315 - recall_m: 0.8312 - precision_m: 0.8568 - f1_m: 0.8039\n",
      "Epoch 5/30\n",
      " - 3s - loss: 0.4137 - acc: 0.7949 - mse: 0.1343 - recall_m: 0.8162 - precision_m: 0.8889 - f1_m: 0.8233\n",
      "\n",
      "Epoch 00005: loss improved from inf to 0.41369, saving model to C:/Nitesh/Projects/data/ImageClassification/rop_model.hdf5\n",
      "Epoch 6/30\n",
      " - 3s - loss: 0.5562 - acc: 0.8077 - mse: 0.1321 - recall_m: 0.8120 - precision_m: 0.8846 - f1_m: 0.8144\n",
      "Epoch 7/30\n",
      " - 3s - loss: 0.3105 - acc: 0.8397 - mse: 0.1033 - recall_m: 0.8868 - precision_m: 0.8932 - f1_m: 0.8725\n",
      "Epoch 8/30\n",
      " - 3s - loss: 0.3652 - acc: 0.8269 - mse: 0.1164 - recall_m: 0.8504 - precision_m: 0.8868 - f1_m: 0.8398\n",
      "Epoch 9/30\n",
      " - 3s - loss: 0.2999 - acc: 0.8590 - mse: 0.0997 - recall_m: 0.8333 - precision_m: 0.8697 - f1_m: 0.8342\n",
      "Epoch 10/30\n",
      " - 3s - loss: 0.2883 - acc: 0.8782 - mse: 0.0925 - recall_m: 0.9145 - precision_m: 0.8953 - f1_m: 0.8882\n",
      "\n",
      "Epoch 00010: loss improved from 0.41369 to 0.28826, saving model to C:/Nitesh/Projects/data/ImageClassification/rop_model.hdf5\n",
      "Epoch 11/30\n",
      " - 3s - loss: 0.2300 - acc: 0.8846 - mse: 0.0795 - recall_m: 0.8761 - precision_m: 0.9338 - f1_m: 0.8856\n",
      "Epoch 12/30\n",
      " - 3s - loss: 0.2382 - acc: 0.8782 - mse: 0.0796 - recall_m: 0.8910 - precision_m: 0.8974 - f1_m: 0.8736\n",
      "Epoch 13/30\n",
      " - 3s - loss: 0.2336 - acc: 0.9038 - mse: 0.0738 - recall_m: 0.8996 - precision_m: 0.9252 - f1_m: 0.8944\n",
      "Epoch 14/30\n",
      " - 3s - loss: 0.2212 - acc: 0.8974 - mse: 0.0707 - recall_m: 0.9145 - precision_m: 0.9444 - f1_m: 0.9110\n",
      "Epoch 15/30\n",
      " - 3s - loss: 0.2797 - acc: 0.8974 - mse: 0.0820 - recall_m: 0.8996 - precision_m: 0.8910 - f1_m: 0.8814\n",
      "\n",
      "Epoch 00015: loss improved from 0.28826 to 0.27974, saving model to C:/Nitesh/Projects/data/ImageClassification/rop_model.hdf5\n",
      "Epoch 16/30\n",
      " - 3s - loss: 0.2227 - acc: 0.8974 - mse: 0.0699 - recall_m: 0.9295 - precision_m: 0.9444 - f1_m: 0.9236\n",
      "Epoch 17/30\n",
      " - 3s - loss: 0.2279 - acc: 0.8974 - mse: 0.0706 - recall_m: 0.9423 - precision_m: 0.9167 - f1_m: 0.9136\n",
      "Epoch 18/30\n",
      " - 3s - loss: 0.1857 - acc: 0.9423 - mse: 0.0546 - recall_m: 0.9722 - precision_m: 0.9615 - f1_m: 0.9563\n",
      "Epoch 19/30\n",
      " - 3s - loss: 0.1650 - acc: 0.9231 - mse: 0.0502 - recall_m: 0.9402 - precision_m: 0.9573 - f1_m: 0.9375\n",
      "Epoch 20/30\n",
      " - 3s - loss: 0.1783 - acc: 0.9038 - mse: 0.0596 - recall_m: 0.9209 - precision_m: 0.9295 - f1_m: 0.9153\n",
      "\n",
      "Epoch 00020: loss improved from 0.27974 to 0.17832, saving model to C:/Nitesh/Projects/data/ImageClassification/rop_model.hdf5\n",
      "Epoch 21/30\n",
      " - 3s - loss: 0.1633 - acc: 0.9487 - mse: 0.0436 - recall_m: 0.9658 - precision_m: 0.9744 - f1_m: 0.9631\n",
      "Epoch 22/30\n",
      " - 3s - loss: 0.2161 - acc: 0.9359 - mse: 0.0549 - recall_m: 0.9701 - precision_m: 0.9509 - f1_m: 0.9482\n",
      "Epoch 23/30\n",
      " - 3s - loss: 0.1060 - acc: 0.9615 - mse: 0.0313 - recall_m: 0.9530 - precision_m: 0.9423 - f1_m: 0.9399\n",
      "Epoch 24/30\n",
      " - 3s - loss: 0.4579 - acc: 0.9615 - mse: 0.0364 - recall_m: 0.9786 - precision_m: 0.9786 - f1_m: 0.9739\n",
      "Epoch 25/30\n",
      " - 3s - loss: 0.1043 - acc: 0.9551 - mse: 0.0319 - recall_m: 0.9637 - precision_m: 0.9786 - f1_m: 0.9651\n",
      "\n",
      "Epoch 00025: loss improved from 0.17832 to 0.10429, saving model to C:/Nitesh/Projects/data/ImageClassification/rop_model.hdf5\n",
      "Epoch 26/30\n",
      " - 3s - loss: 0.1087 - acc: 0.9551 - mse: 0.0317 - recall_m: 0.9722 - precision_m: 0.9658 - f1_m: 0.9659\n",
      "Epoch 27/30\n",
      " - 3s - loss: 0.2056 - acc: 0.9551 - mse: 0.0443 - recall_m: 0.9786 - precision_m: 0.9701 - f1_m: 0.9700\n",
      "Epoch 28/30\n",
      " - 3s - loss: 0.0904 - acc: 0.9551 - mse: 0.0301 - recall_m: 0.9658 - precision_m: 0.9701 - f1_m: 0.9602\n",
      "Epoch 29/30\n",
      " - 3s - loss: 0.0777 - acc: 0.9615 - mse: 0.0249 - recall_m: 0.9722 - precision_m: 0.9701 - f1_m: 0.9613\n",
      "Epoch 30/30\n",
      " - 3s - loss: 0.2114 - acc: 0.9615 - mse: 0.0315 - recall_m: 0.9808 - precision_m: 0.9551 - f1_m: 0.9585\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.10429\n"
     ]
    }
   ],
   "source": [
    "#model training- checkpointed model to ensure that best model is saved during run having minimum comparative loss\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "ntrain = len(inp_X)  #xTrain\n",
    "batch_size = 4\n",
    "train_generator = train_datagen.flow(inp_X, inp_y, batch_size = batch_size)\n",
    "\n",
    "'save model if there is improvement in loss'\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_pathname + \"rop_model.hdf5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=5) # period = number of epochs after which keeping save the model.\n",
    "#give the model name by which it is to be stored \n",
    "\n",
    "\n",
    "\n",
    "history_callback = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=ntrain // batch_size,  \n",
    "      epochs=30,\n",
    "      #validation_data=validation_generator,\n",
    "      #validation_steps=5,  \n",
    "      verbose=2, callbacks=[checkpoint])  # True while running in server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145c78c",
   "metadata": {},
   "source": [
    "# Trace Loss (while model training) and Save Logs\n",
    "\n",
    "We observe that the loss has consistently decreased and was 0.211369 in final epoch. \n",
    "Accuracy improved as loss decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7d125ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>mse</th>\n",
       "      <th>recall_m</th>\n",
       "      <th>precision_m</th>\n",
       "      <th>f1_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.675198</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.163663</td>\n",
       "      <td>0.831196</td>\n",
       "      <td>0.848291</td>\n",
       "      <td>0.796947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.474133</td>\n",
       "      <td>0.775641</td>\n",
       "      <td>0.153034</td>\n",
       "      <td>0.775641</td>\n",
       "      <td>0.908120</td>\n",
       "      <td>0.801587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.349711</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.115868</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.837363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.398955</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.131543</td>\n",
       "      <td>0.831197</td>\n",
       "      <td>0.856838</td>\n",
       "      <td>0.803907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.413690</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.134278</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.823321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.556214</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.132120</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.814408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.310454</td>\n",
       "      <td>0.839744</td>\n",
       "      <td>0.103338</td>\n",
       "      <td>0.886752</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.872528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.365196</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.116424</td>\n",
       "      <td>0.850427</td>\n",
       "      <td>0.886752</td>\n",
       "      <td>0.839805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.299930</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.099746</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.869658</td>\n",
       "      <td>0.834188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.288257</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.092495</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.895299</td>\n",
       "      <td>0.888156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.230004</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.079509</td>\n",
       "      <td>0.876068</td>\n",
       "      <td>0.933761</td>\n",
       "      <td>0.885592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.238153</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.079583</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.873626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.233597</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.073796</td>\n",
       "      <td>0.899573</td>\n",
       "      <td>0.925214</td>\n",
       "      <td>0.894383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.221180</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.070705</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.944445</td>\n",
       "      <td>0.910989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.279736</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.899573</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>0.881441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.222679</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.069899</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.923565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.227933</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.070579</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.913553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.185741</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.054622</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.956288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.165013</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.050180</td>\n",
       "      <td>0.940171</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>0.937485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.178320</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.059590</td>\n",
       "      <td>0.920940</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.915262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.163341</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.043601</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.963126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.216128</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.054928</td>\n",
       "      <td>0.970086</td>\n",
       "      <td>0.950855</td>\n",
       "      <td>0.948230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.106016</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.031294</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.939927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.457872</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.036401</td>\n",
       "      <td>0.978632</td>\n",
       "      <td>0.978632</td>\n",
       "      <td>0.973871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.104291</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.031853</td>\n",
       "      <td>0.963675</td>\n",
       "      <td>0.978633</td>\n",
       "      <td>0.965079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.108742</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.031662</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.965934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.205577</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.044276</td>\n",
       "      <td>0.978633</td>\n",
       "      <td>0.970086</td>\n",
       "      <td>0.969963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.090392</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.030122</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>0.960195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.077738</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.024853</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.970085</td>\n",
       "      <td>0.961294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.211369</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.031526</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.955128</td>\n",
       "      <td>0.958486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       acc       mse  recall_m  precision_m      f1_m\n",
       "0   0.675198  0.743590  0.163663  0.831196     0.848291  0.796947\n",
       "1   0.474133  0.775641  0.153034  0.775641     0.908120  0.801587\n",
       "2   0.349711  0.833333  0.115868  0.826923     0.891026  0.837363\n",
       "3   0.398955  0.782051  0.131543  0.831197     0.856838  0.803907\n",
       "4   0.413690  0.794872  0.134278  0.816239     0.888889  0.823321\n",
       "5   0.556214  0.807692  0.132120  0.811966     0.884615  0.814408\n",
       "6   0.310454  0.839744  0.103338  0.886752     0.893162  0.872528\n",
       "7   0.365196  0.826923  0.116424  0.850427     0.886752  0.839805\n",
       "8   0.299930  0.858974  0.099746  0.833333     0.869658  0.834188\n",
       "9   0.288257  0.878205  0.092495  0.914530     0.895299  0.888156\n",
       "10  0.230004  0.884615  0.079509  0.876068     0.933761  0.885592\n",
       "11  0.238153  0.878205  0.079583  0.891026     0.897436  0.873626\n",
       "12  0.233597  0.903846  0.073796  0.899573     0.925214  0.894383\n",
       "13  0.221180  0.897436  0.070705  0.914530     0.944445  0.910989\n",
       "14  0.279736  0.897436  0.081974  0.899573     0.891026  0.881441\n",
       "15  0.222679  0.897436  0.069899  0.929487     0.944444  0.923565\n",
       "16  0.227933  0.897436  0.070579  0.942308     0.916667  0.913553\n",
       "17  0.185741  0.942308  0.054622  0.972222     0.961538  0.956288\n",
       "18  0.165013  0.923077  0.050180  0.940171     0.957265  0.937485\n",
       "19  0.178320  0.903846  0.059590  0.920940     0.929487  0.915262\n",
       "20  0.163341  0.948718  0.043601  0.965812     0.974359  0.963126\n",
       "21  0.216128  0.935897  0.054928  0.970086     0.950855  0.948230\n",
       "22  0.106016  0.961538  0.031294  0.952991     0.942308  0.939927\n",
       "23  0.457872  0.961538  0.036401  0.978632     0.978632  0.973871\n",
       "24  0.104291  0.955128  0.031853  0.963675     0.978633  0.965079\n",
       "25  0.108742  0.955128  0.031662  0.972222     0.965812  0.965934\n",
       "26  0.205577  0.955128  0.044276  0.978633     0.970086  0.969963\n",
       "27  0.090392  0.955128  0.030122  0.965812     0.970085  0.960195\n",
       "28  0.077738  0.961538  0.024853  0.972222     0.970085  0.961294\n",
       "29  0.211369  0.961538  0.031526  0.980769     0.955128  0.958486"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss_history = history_callback.history\n",
    "\n",
    "lossdata = pd.DataFrame.from_dict(loss_history)\n",
    "\n",
    "lossdata.to_csv(checkpoint_pathname + 'losshistory.csv')\n",
    "lossdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c148b",
   "metadata": {},
   "source": [
    "# Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb1b9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "     'recall_m': recall_m, \n",
    "     'precision_m': precision_m,\n",
    "     'f1_m': f1_m,\n",
    "}\n",
    "\n",
    "model1 = load_model(model_pathname + 'rop_model.hdf5', custom_objects=dependencies) # load the trained model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f3c76",
   "metadata": {},
   "source": [
    "#  Read, Preprocess and Prediction on the Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5398db5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of Test Images:\n",
      " 24\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0AElEQVR4nO3deZzN9f7A8dfbjJJCsmXft5nJlZSQdbIn3FJK0r2DECltWkguSklly5JsyZISbkqluvq5SbbECJPsgzExlpAZ798f5ztzx5jhYM75zjnn/Xw8zsN3+ZzzfX8N5z2fz+f7fX9FVTHGGBO6crkdgDHGGHdZIjDGmBBnicAYY0KcJQJjjAlxlgiMMSbEWSIwxpgQZ4nAGGNCnCUCE3REZIeInBSR4yKyX0Smich1GdrUE5FvROSYiCSJyGIRicjQJr+IvC0iu5zP+s1ZL+zfMzLGtywRmGDVVlWvA2oCNwPPp+4QkbrAl8BCoARQHvgZWCEiFZw2VwHLgEigJZAfqAskArf5KmgRCffVZxuTFUsEJqip6n5gKZ6EkOp1YIaqvqOqx1T1D1V9CVgJDHbaPAyUATqoaqyqnlXVg6r6L1VdktmxRCRSRL4SkT9E5ICIvOBsnyYiQ9O1aywie9Kt7xCR50RkA3DCWZ6f4bPfEZHRznIBEZkiIvEisldEhopI2JX9TZlQZonABDURKQW0AuKc9bxAPeCjTJrPA5o5y3cCX6jqcS+Pkw/4GvgCTy+jEp4ehbceANoA1wNzgNbOZ+J8yd8HfOi0nQYkO8e4GWgOdLuEYxlzDksEJlh9KiLHgN3AQeBlZ/sNeP7dx2fynnggdfy/UBZtsnIXsF9V31TVU05P48dLeP9oVd2tqidVdSewFujg7GsK/KmqK0WkGNAaeEJVT6jqQeAtoNMlHMuYc1giMMGqvarmAxoD1fjfF/xh4CxQPJP3FAcOOcuJWbTJSmngt8uK1GN3hvUP8fQSAB7kf72BskBuIF5EjojIEWAiUPQKjm1CnCUCE9RU9T94hlJGOusngB+Ajpk0v4//Ded8DbQQkWu9PNRuoEIW+04AedOt35hZqBnWPwIaO0NbHfhfItgNnAYKq+r1ziu/qkZ6Gacx57FEYELB20AzEfmbsz4A6Coij4tIPhEp6Ezm1gVecdrMxPOl+7GIVBORXCJSSEReEJHWmRzj30BxEXlCRK52PreOs289njH/G0TkRuCJiwWsqgnAd8BU4HdV3exsj8dzxdObzuWtuUSkoog0utS/FGNSWSIwQc/5Up0BDHLW/w9oAfwdzzzATjyTrneo6janzWk8E8a/Al8BR4FVeIaYzhv7V9VjeCaa2wL7gW1AE2f3TDyXp+7A8yU+18vQP3Ri+DDD9oeBq4BYPENd87m0YSxjziH2YBpjjAlt1iMwxpgQZ4nAGGNCnCUCY4wJcZYIjDEmxAVcgavChQtruXLl3A7DGGMCypo1aw6papHM9gVcIihXrhyrV692OwxjjAkoIrIzq302NGSMMSHOEoExxoQ4SwTGGBPiAm6OIDNnzpxhz549nDp1yu1QTA6XJ08eSpUqRe7cud0OxZgcIygSwZ49e8iXLx/lypVDRNwOx+RQqkpiYiJ79uyhfPnybodjTI7hs6EhEXlfRA6KyMYs9ouIjBaROBHZICK1LvdYp06dolChQpYEzAWJCIUKFbKeozEZ+HKOYBqeh35npRVQ2Xn1AN69koNZEjDesH8nxpzPZ4lAVZcDf1ygSTs8DxBXVV0JXC8iVkrXGGMyOHHiBE/N+oFXFm/yyee7edVQSc59PN8eZ9t5RKSHiKwWkdUJCQl+Cc4YY3KCb775hho1arDwP2uI3XfUJ8cIiMtHVXWSqtZW1dpFimR6h7TrrrvuurTlJUuWUKVKFXbuzPJGvmy1bt06YmJi/HKsy3H69Gnuv/9+KlWqRJ06ddixY8d5bbZs2ULNmjXTXvnz5+ftt98G4I8//qBZs2ZUrlyZZs2acfjwYcAz+fv4449TqVIlatSowdq1awFISEigZcsLjUoak/MdOXKE7t27Ex0dTa5cuahUqaLPjuVmItiL54HfqUo52wLasmXLePzxx/n8888pW7asV+9JSUm5omMOHz6cxx9/3Ov2ycnJV3S8SzVlyhQKFixIXFwcTz75JM8999x5bapWrcr69etZv349a9asIW/evHTo0AGA1157jejoaLZt20Z0dDSvvfYaAJ9//jnbtm1j27ZtTJo0iV69egFQpEgRihcvzooVK/x3ksZko5SUFOrVq8f777/Ps88+y4YNGyhQ4HqfHc/Ny0cXAX1EZA5QB0hynsd6RV5ZvCnbu08RJfLzctuLPxt8+fLldO/enSVLllCxoid7f/DBB4wePZq//vqLOnXqMH78eMLCwrjuuut49NFH+frrrxk3bhzffPMNixcv5uTJk9SrV4+JEyciIowePZoJEyYQHh5OREQEc+bMOeeYx44dY8OGDfztb57H8a5atYp+/fpx6tQprrnmGqZOnUrVqlWZNm0an3zyCcePHyclJYUlS5bQt29fNm7cyJkzZxg8eDDt2rVjx44ddOnShRMnTgAwduxY6tWrd0V/fwsXLmTw4MEA3HvvvfTp0wdVzXLidtmyZVSsWDEtkS5cuJDvvvsOgK5du9K4cWNGjBjBwoULefjhhxERbr/9do4cOUJ8fDzFixenffv2zJo1i/r1619R7Mb4U2JiIjfccANhYWEMGzaM0qVLU7t2bZ8f15eXj84GfgCqisgeEYkRkZ4i0tNpsgTYDsQBk4HevorFH06fPk379u359NNPqVatGgCbN29m7ty5rFixgvXr1xMWFsasWbMAz+RPnTp1+Pnnn7njjjvo06cPP/30Exs3buTkyZP8+9//Bjy/Da9bt44NGzYwYcKE8467evVqoqKi0tarVavG999/z7p16xgyZAgvvPBC2r61a9cyf/58/vOf/zBs2DCaNm3KqlWr+Pbbb3nmmWc4ceIERYsW5auvvmLt2rXMnTs3y55GgwYNzhnKSX19/fXX57Xdu3cvpUt7On/h4eEUKFCAxMTELP8u58yZwwMPPJC2fuDAAYoX91xHcOONN3LgwIHzPhegVKlS7N3r6VTWrl2b77//PstjGJOTqCoffPABVapU4b333gOgQ4cOfkkC4MMegao+cJH9CjyW3cf15jd3X8idOzf16tVjypQpvPPOO4DnN9s1a9Zw6623AnDy5EmKFi0KQFhYGPfcc0/a+7/99ltef/11/vzzT/744w8iIyNp27YtNWrUoHPnzrRv35727dufd9z4+HjSz5skJSXRtWtXtm3bhohw5syZtH3NmjXjhhtuAODLL79k0aJFjBw5EvDci7Fr1y5KlChBnz590hLX1q1bMz1fX33J/vXXXyxatIhXX3010/0i4tUloEWLFmXfvn3ZHZ4x2W737t307NmTJUuWcPvtt7vSiw2KO4tzgly5cjFv3jyio6MZPnw4L7zwAqpK165dM/1Sy5MnD2FhYYDnS7h3796sXr2a0qVLM3jw4LSbnj777DOWL1/O4sWLGTZsGL/88gvh4f/7sV1zzTXn3CA1cOBAmjRpwoIFC9ixYweNGzdO23fttdemLasqH3/8MVWrVj0nrsGDB1OsWDF+/vlnzp49S548eTI93wYNGnDs2LHzto8cOZI777zznG0lS5Zk9+7dlCpViuTkZJKSkihUqFCmn/v5559Tq1YtihUrlratWLFiaUM+8fHxack09XNT7dmzh5IlS6b9nV5zzTWZHsOYnGL27Nk8+uijpKSk8Pbbb9OnT5+07wV/CoirhgJF3rx5+eyzz5g1axZTpkwhOjqa+fPnc/DgQcBz9UtmVxKlfpEXLlyY48ePM3/+fADOnj3L7t27adKkCSNGjCApKYnjx4+f897q1asTFxeXtp6UlJT2ZTht2rQsY23RogVjxozB0zHzXHmU+v7ixYuTK1cuZs6cmeVE9vfff582uZv+lTEJANx9991Mnz4dgPnz59O0adMsf6ufPXv2OcNCGd8/ffp02rVrl7Z9xowZqCorV66kQIECaUNIW7duPWfIzJicqGDBgtSpU4eNGzfSr18/V5IAWCLIdjfccANffPEFQ4cOJS4ujqFDh9K8eXNq1KhBs2bNiI8/fz78+uuvp3v37kRFRdGiRYu0oaSUlBQeeughbrrpJm6++WYef/xxrr/++nPeW61aNZKSktJ+O3/22Wd5/vnnufnmmy94ddDAgQM5c+YMNWrUIDIykoEDBwLQu3dvpk+fzt/+9jd+/fXXc3oRlysmJobExEQqVarEqFGj0q762bdvH61bt05rd+LECb766iv+/ve/n/P+AQMG8NVXX1G5cmW+/vprBgwYAEDr1q2pUKEClSpVonv37owfPz7tPd9++y1t2rS54tiNyU7Jycm88cYbDBs2DICWLVvy5Zdful77SlJ/IwwUtWvX1oxPKNu8eTPVq1d3KSL3vfXWW+TLl49u3bq5HUqO0bBhQxYuXEjBggXP2xfq/16MO37++WdiYmJYs2YN9913H3PmzLmkkif3T/wBgLmP1r2s44vIGlXNdPbZegRBoFevXlx99dVuh5FjJCQk0L9//0yTgDH+dvr0aQYOHEjt2rXZvXs3H3300SUnAV8LmkQQaD2b7JQnTx66dOnidhg5RpEiRTK9wgpC+9+Jcce2bdsYMWIEDz74ILGxsdx77705KglAkCSCPHnykJiYaP/JzQWlPo8gqyuhjMkux48fT7tnKCoqil9//ZXp06dnebWc24Li8tFSpUqxZ88erCCduZjUJ5SZK/fhj7tYuD7gq8Jku8OHD7N161ZOnTrFnIPLyJs3r7PnwBV9bmz8USKK57/yADMRFIkgd+7crs+6GxNqFq7f69Mvp0CTnJzMb7/9xv79+7nmmmuoWbNmuiRw5SKK56ddzUwLNF+xoEgExhh3RBTPf9lXsQSTlJQUbrrpJrZu3cqzzz7LoEFPBNQQpCUCY4y5TIcOHUorEjd8+HDKlClDrVqX/dRd1wTFZLExxviTqjJjxoxzisS1b98+IJMAWCIwxphLsnPnTlq1akXXrl2pXr06DRs2dDukK2aJwBhjvPTBBx8QFRXF//3f/zFmzBi+//77tLLzgczmCIwxxktFihShfv36TJw40esnEAYCSwTGGJOFM2fO8Oabb3LmzBkGDhxIixYtaN68eY67M/hK2dCQMcZkYt26ddSpU4fnn3+e2NjYtMoFwZYEwBKBMcac49SpU7zwwgvceuut7Nu3j48//pjZs2cHZQJIZYnAGGPSiYuLY+TIkTz88MNs3rz5vOdjBCObIzDGhLzjx4+zYMECunTpQlRUFFu2bAmpsjXWIzDGhLSlS5cSGRlJ165d2bx5M0BIJQGwHoExAc+tKqCBXnAuMTGR/v37M2PGDKpVq8b3338fsk+us0RgTIBzqwqoL6th+lpKSgr169cnLi6OF198kZdeeimgisRlN0sExgQBqwLqnYSEBAoVKkRYWBgjRoygbNmy1KxZ0+2wXGdzBMaYoKeqTJ06lSpVqjB58mQA2rVrZ0nAYYnAGBPUduzYQYsWLfjnP//JTTfdRJMmTdwOKcexRGCMCVozZ84kKiqKH374gfHjx/Pdd99RpUoVt8PKcWyOwBgTtIoVK0bDhg2ZMGECZcqUcTucHMsSgTEmaJw5c4bXX3+dlJQUBg0aRPPmzWnevLnbYeV4NjRkjAkKa9eu5dZbb+Wll15iy5YtaUXizMVZIjDGBLSTJ08yYMAAbrvtNg4cOMCCBQuYNWtWUBeJy24+TQQi0lJEtohInIgMyGR/GRH5VkTWicgGEWnty3iMMcFn+/btjBo1ikceeYTY2Fjat2/vdkgBx2eJQETCgHFAKyACeEBEIjI0ewmYp6o3A52A8b6KxxgTPI4ePcq0adMAiIyMZNu2bbz33nsULFjQ3cAClC97BLcBcaq6XVX/AuYA7TK0USD1vvgCwD4fxmOMCQJLliwhKiqKmJiYtCJxwfTYSDf4MhGUBHanW9/jbEtvMPCQiOwBlgB9M/sgEekhIqtFZHVCQoIvYjXG5HCHDh2iS5cutGnThnz58rFixYqQLRKX3dyeLH4AmKaqpYDWwEwROS8mVZ2kqrVVtXaRIkX8HqQxxl2pReLmzJnDoEGDWLt2LbfffrvbYQUNX95HsBconW69lLMtvRigJYCq/iAieYDCwEEfxmX8wK3SyKEo0MtBX8iBAwcoUqQIYWFhjBw5krJly1KjRg23wwo6vuwR/ARUFpHyInIVnsngRRna7AKiAUSkOpAHsLGfIJBaGtn4XiCXg86KqjJlyhSqVq3KpEmTAGjbtq0lAR/xWY9AVZNFpA+wFAgD3lfVTSIyBFitqouAp4DJIvIknonjR9TuAgkaVhrZXI7t27fTvXt3vvnmGxo1asSdd97pdkhBz6clJlR1CZ5J4PTbBqVbjgXq+zIGY0zgmD59Or179yYsLIwJEybQvXt3cuVyeyoz+FmtIWNMjlGiRAmaNm3Ku+++S6lSpdwOJ2RYIjDGuOavv/7itdde4+zZswwePJhmzZrRrFkzt8MKOdbnMsa44qeffuKWW27h5ZdfZvv27VYkzkWWCIwxfvXnn3/y9NNPc/vtt3P48GEWLVrEjBkzrEiciywRGGP86vfff2fMmDF0796dTZs20bZtW7dDCnk2R2CM8bmkpCQ++eQT/vGPfxAZGUlcXBylS5e++BuNX1iPwBjjU5999hmRkZF069aNX3/9FcCSQA5jicAY4xMJCQl07tyZu+66i4IFC/LDDz9QrVo1t8MymbChIWNMtktJSeGOO+7g999/55VXXmHAgAFcddVVbodlsmCJwBiTbfbv30/RokUJCwvjzTffpFy5ckRFRbkdlrkISwRBzM0KoMFcEdOc7+zZs0yePJlnnnmGESNG0KtXL+666y63wzJe8mqOQESuEZGqvg7GZC83K4AGY0VMk7m4uDiio6Pp2bMnt956Ky1atHA7JHOJLtojEJG2wEjgKqC8iNQEhqjq3T6OzWQDqwBqfGnq1Kn07t2bq666ismTJxMTE2M3hgUgb3oEg/E8f/gIgKquB8r7LCJjTMAoU6YMLVq0IDY2lm7dulkSCFDezBGcUdWkDD9gKwpiTAg6ffo0r776KmfPnmXIkCFER0cTHR3tdljmCnnTI9gkIg8CYSJSWUTGAP/1cVzGmBzmxx9/5JZbbuGVV15h165dViQuiHiTCPoCkcBp4EMgCejny6CMMTnHiRMn6N+/P3Xr1iUpKYl///vfTJs2zYaBgog3iaCNqr6oqrc6r5cAmyg2JkTs3LmT8ePH07NnTzZt2kSbNm3cDslkM28SwfNebjPGBIkjR47w3nvvARAREUFcXBzjx48nf367NyQYZTlZLCKtgNZASREZnW5XfiDZ14EZY9yxcOFCevXqxcGDB7njjjuoVq2aPTYyyF2oR7APWA2cAtakey0C7I4RY4LMwYMH6dSpE+3bt6dIkSKsXLnSisSFiCx7BKr6M/CziHyoqmf8GJMxxs9SUlKoX78+u3btYujQoTz77LPkzp3b7bCMn3hzH0E5EXkViADypG5U1Qo+i8oY4xf79u3jxhtvJCwsjHfeeYdy5coRERHhdljGz7yZLJ4KvItnXqAJMAP4wJdBGWN86+zZs7z77rtUq1aNCRMmANC6dWtLAiHKm0RwjaouA0RVd6rqYMCuHzMmQG3dupUmTZrQu3dv6tSpQ6tWrdwOybjMm6Gh0yKSC9gmIn2AvcB1vg3LGOMLU6ZMoU+fPuTJk4f333+fRx55xG4MM171CPoBeYHHgVuAh4CuvgzKGOMb5cqVo1WrVsTGxvKPf/zDkoABLtIjEJEw4H5VfRo4DvzDL1EZY7LF6dOn+de//gXA0KFDrUicydQFewSqmgLc4adYjDHZ6L///S81a9Zk2LBhxMfHW5E4kyVv5gjWicgi4CPgROpGVf3EZ1EZYy7b8ePHefHFFxkzZgylS5fmiy++sKeGmQvyZo4gD5AINAXaOi+vHkYqIi1FZIuIxInIgCza3CcisSKySUQ+9DZwY0zmdu3axcSJE3nsscfYuHGjJQFzURftEajqZc0LOPML44BmwB7gJxFZpKqx6dpUxlPArr6qHhaRopdzLGNC3eHDh/noo4/o0aMHERERbN++nRIlSrgdlgkQXj28/jLdBsSp6nZV/QuYA7TL0KY7ME5VDwOo6kEfxmNMUFqwYAERERH07t2bLVu2AFgSMJfEl4mgJLA73foeZ1t6VYAqIrJCRFaKSMvMPkhEeojIahFZnZCQ4KNwjQks+/fvp2PHjvz973/nxhtvZNWqVVStWtXtsEwA8may2NfHrww0BkoBy0XkJlU9kr6Rqk4CJgHUrl3bLn0wIS8lJYUGDRqwe/duhg8fztNPP21F4sxlu2giEJFiwHCghKq2EpEIoK6qTrnIW/cCpdOtl3K2pbcH+NGpbvq7iGzFkxh+8vYEjAkle/bsoUSJEoSFhTF69GjKly9vpaLNFfNmaGgasBRIHXTcCjzhxft+AiqLSHkRuQrohOdZBul9iqc3gIgUxjNUtN2LzzYmpJw9e5YxY8ZQrVo13n33XQBatWplScBkC28SQWFVnQecBVDVZCDlYm9y2vXBk0Q2A/NUdZOIDBGR1GceLwUSRSQW+BZ4RlUTL+M8jAlav/76Kw0bNuTxxx/njjvu4K67vLp62xiveTNHcEJECgEKICK3A0nefLiqLgGWZNg2KN2yAv2dlzEmg/fee48+ffqQN29epk+fTpcuXaw+kMl23iSCp/AM6VQUkRVAEeBen0ZljAGgYsWKtG3blrFjx1KsWDG3wzFBypsbytaISCOgKiDAFnt0pTG+cerUKYYMGQLA8OHDadKkCU2aNHE5KhPsLjpHICIbgGeBU6q60ZKAMb6xYsUKatasyauvvkpCQoIViTN+481kcVs8j6mcJyI/icjTIlLGx3EZEzKOHTtG3759adCgAadPn2bp0qVMnjzZ5gKM31w0ETiPp3xdVW8BHgRqAL/7PDJjQsSePXt477336Nu3L7/88gvNmzd3OyQTYry6s1hEygL3O68UPENFxpjLlJiYyLx58+jVqxfVq1dn+/btFC9e3O2wTIjy5s7iH4HceJ5H0FFV7YYvYy6TqvLxxx/z2GOP8ccff9C0aVOqVq1qScC4yps5godVtZaqvmpJwJjLFx8fzz333EPHjh0pXbo0q1evtiJxJkfIskcgIg+p6gdAGxFpk3G/qo7yaWTGBJHUInF79+7l9ddf58knnyQ83O2aj8Z4XOhf4rXOn/ky2WfXtRnjhd27d1OyZEnCwsIYN24c5cuXp0qVKm6HZcw5shwaUtWJzuLXqvpK+hewzD/hGROYUlJSGD169DlF4lq0aGFJwORI3swRjPFymzEG2Lx5Mw0aNKBfv340atSItm3buh2SMRd0oTmCukA9oIiIpC8Klx8I83VgxgSiSZMm0bdvX/Lly8fMmTPp3Lmz3RhmcrwLzRFcBVzntEk/T3AUKzpnTKYqV65Mhw4dGD16NEWLFnU7HGO8kmUiUNX/AP8RkWmqutOPMRkTME6ePMngwYMREV577TUrEmcC0oWGht5W1SeAsSJy3lVCqnr3+e8yJnQsX76cbt26sW3bNnr27Imq2jCQCUgXGhqa6fw50h+BGBMojh49yoABA3j33XepUKECy5Yto2nTpm6HZcxlu9DQ0Brnz/+kbhORgkBpVd3gh9iMyZH27dvHtGnT6N+/P0OGDOHaa6+9+JuMycG8qTX0HXC303YNcFBEVqiqPV7ShIxDhw4xb948evfuTbVq1fj999/tiWEmaHhzH0EBVT0K/B2Yoap1gDt9G5YxOYOqMnfuXCIiInjiiSfYunUrgCUBE1S8SQThIlIcuA/4t4/jMSbH2LdvH+3bt6dTp06ULVuWNWvW2J3BJih5U/VqCLAUWKGqP4lIBWCbb8Myxl0pKSk0bNiQvXv3MnLkSPr162dF4kzQ8ubh9R/heRZB6vp24B5fBmWMW3bu3EmpUqUICwtj/PjxVKhQgUqVKrkdljE+5c3D60uJyAIROei8PhaRUv4Izhh/SUlJYdSoUVSvXj2tSFzz5s0tCZiQ4M0cwVRgEVDCeS12thkTFDZu3Ei9evV46qmniI6Opn379m6HZIxfeZMIiqjqVFVNdl7TgCI+jssYv5gwYQK1atVi+/btfPjhhyxatIhSpazDa0KLN4kgUUQeEpEw5/UQkOjrwIzxJVVP1ZTq1avTsWNHYmNjeeCBB6xEhAlJ3lwG8U88zx94y1lfAfzDZxEZ40N//vkngwYNIiwsjBEjRtCoUSMaNWrkdljGuOqiPQJV3amqd6tqEefVXlV3+SM4Y7LTd999R40aNXjzzTc5fvx4Wq/AmFDnzVVDFURksYgkOFcNLXTuJTAmICQlJfHoo4+mlYf+5ptvGDdunA0DGePwZo7gQ2AeUBzPVUMfAbN9GZQx2Sk+Pp4PPviAp59+mg0bNtjzAozJwJtEkFdVZ6a7augDII83Hy4iLUVki4jEiciAC7S7R0RURGp7G7gxF5KQkMCYMZ5Ha1erVo0dO3bwxhtvkDdvXpcjMybn8SYRfC4iA0SknIiUFZFngSUicoOI3JDVm0QkDBgHtAIigAdEJCKTdvmAfsCPl3cKxvyPqvLhhx9SvXp1nnrqqbQicUWK2BXPxmTFm0RwH/Ao8C3wHdAL6ISnJPXqC7zvNiBOVber6l/AHKBdJu3+BYwATnkftjHn2717N23btqVz585UqlSJdevWWZE4Y7zgTa2h8pf52SWB3enW9wB10jcQkVp4HnTzmYg8k9UHiUgPoAdAmTJlLjMcE8ySk5Np3Lgx+/fv56233qJv376EhYW5HZYxAcG1cooikgsYBTxysbaqOgmYBFC7dm275s+k2bFjB6VLlyY8PJyJEydSoUIFKlSwi9qMuRTeDA1drr1A6XTrpZxtqfIBUcB3IrIDuB1YZBPGxhvJycmMHDmS6tWrM378eADuvPNOSwLGXAZf9gh+AiqLSHk8CaAT8GDqTlVNAgqnrjuPxHxaVS8072AMGzZsICYmhtWrV9OuXTvuuceqohtzJby5oUycWkODnPUyInLbxd6nqslAHzwPtdkMzFPVTSIyRETuvtLATWgaP348t9xyCzt37mTu3LksWLCAEiVKuB2WMQHNmx7BeOAs0BTP08qOAR8Dt17sjaq6BFiSYdugLNo29iIWE6JUFREhKiqKTp068dZbb1G4cOGLv9EYc1HeJII6qlpLRNYBqOphEbnKx3EZA8CJEyd46aWXCA8P54033qBhw4Y0bNjQ7bCMCSreTBafcW4OUwARKYKnh2CMTy1btoybbrqJt99+m9OnT1uROGN8xJtEMBpYABQVkWHA/wHDfRqVCWlHjhyhW7du3HnnnYSHh7N8+XJGjx5tReKM8RFvbiibJSJrgGhAgPaqutnnkZmQdeDAAebMmcNzzz3Hyy+/zDXXXON2SMYEtYsmAhEpA/yJ51nFadvsmQQmO6V++ffr14+qVauyY8cOmww2xk+8mSz+DM/8gOCpOloe2AJE+jAuEyJUlVmzZtGvXz+OHz9O69atqVy5siUBY/zImyeU3aSqNZw/K+MpJveD70MzwW7Xrl20adOGLl26ULVqVdavX0/lypXdDsuYkHPJdxar6loRqXPxlsZkLbVI3MGDBxk9ejS9e/e2InHGuMSbOYL+6VZzAbWAfT6LyAS17du3U7ZsWcLDw5k8eTIVK1akXLlybodlTEjz5vLRfOleV+OZM8jsuQLGZCk5OZkRI0YQERHBuHHjAIiOjrYkYEwOcMEegXMjWT5VfdpP8ZggtH79emJiYli7di0dOnSgY8eObodkjEknyx6BiISragpQ34/xmCAzduxYbr31Vvbu3cv8+fP55JNPKF68uNthGWPSuVCPYBWe+YD1IrII+Ag4kbpTVT/xcWwmgKUWiatRowadO3dm1KhR3HBDlo+4Nsa4yJurhvIAiXiqj6beT6CAJQJznuPHj/Piiy+SO3duRo4caUXijAkAF5osLupcMbQR+MX5c5Pz50Y/xGYCzJdffklUVBRjxozhzJkzViTOmABxoR5BGHAdnh5ARvY/3KQ5fPgw/fv3Z9q0aVStWpXly5dzxx13uB2WMcZLF0oE8ao6xG+RmIB18OBB5s+fz/PPP8+gQYPIkyeP2yEZYy7BhRKB1fw1Wdq/fz+zZ8/mySefTCsSV6hQIbfDMsZchgvNEUT7LQoTMFSV6dOnExERwfPPP8+2bdsALAkYE8CyTASq+oc/AzE5344dO2jZsiWPPPIIERERViTOmCBxyUXnTGhKTk6mSZMmHDp0iHHjxtGzZ09y5fKmQokxJqezRGAuKC4ujvLlyxMeHs77779PhQoVKFu2rNthGWOykf1KZzJ15swZhg8fTmRkZFqRuCZNmlgSMCYIWY/AnGft2rXExMSwfv16OnbsyP333+92SMYYH7IegTnH6NGjue2229i/fz+ffPIJ8+bNo1ixYm6HZYzxIUsEBiCtHMTNN9/Mww8/TGxsLB06dHA5KmOMP9jQUIg7duwYzz//PFdffTVvvvkmDRo0oEGDBm6HZYzxI+sRhLAvvviCqKgoxo8fj6pakThjQpQlghCUmJhI165dadWqFddeey0rVqxg1KhRiFhVEWNCkSWCEJSYmMiCBQsYOHAg69ato27dum6HZIxxkU/nCESkJfAOnpLW76nqaxn29we6AclAAvBPVd3py5jc8OGPu1i4fq/fjxsbf5SI4vkBiI+PZ9asWTz11FNUqVKFnTt3UrBgQb/HZIzJeXzWI3AefD8OaAVEAA+ISESGZuuA2qpaA5gPvO6reNy0cP1eYuOP+v24EcXzc3fNErz//vtUr16dgQMHEhcXB2BJwBiTxpc9gtuAOFXdDiAic4B2QGxqA1X9Nl37lcBDPozHVRHF8zP3Uf8Owfz+++/06NGNr7/+moYNGzJ58mQrEmeMOY8vE0FJYHe69T1AnQu0jwE+z2yHiPQAegCUKVMmu+ILasnJyTRt2pTExETeffddevToYUXijDGZyhH3EYjIQ0BtoFFm+1V1EjAJoHbt2naN4wVs27aNChUqEB4eztSpU6lYsSKlS5d2OyxjTA7my18R9wLpv4FKOdvOISJ3Ai8Cd6vqaR/GE9TOnDnD0KFDiYqKYuzYsQA0btzYkoAx5qJ82SP4CagsIuXxJIBOwIPpG4jIzcBEoKWqHvRhLEFt9erVxMTEsGHDBjp16sQDDzzgdkjGmADisx6BqiYDfYClwGZgnqpuEpEhInK30+wN4DrgIxFZLyKLfBVPsHrnnXeoU6cOhw4dYuHChcyePZuiRYu6HZYxJoD4dI5AVZcASzJsG5Ru+U5fHj+YqSoiQu3atYmJieH111/n+uuvdzssY0wAyhGTxcZ7R48e5bnnniNPnjy89dZb1K9fn/r167sdljEmgNn1hAFkyZIlREZGMmnSJMLDw61InDEmW1giCACHDh3ioYceok2bNhQoUID//ve/vPHGG1YkzhiTLSwRBIDDhw+zePFiXn75ZdauXUudOhe6L88YYy6NzRHkUHv37mXWrFk888wzVK5cmZ07d9pksDHGJ6xHkMOoKpMnTyYiIoLBgwfz22+/AVgSMMb4jCWCHOS3334jOjqaHj16UKtWLTZs2EClSpXcDssYE+RsaCiHSE5OJjo6mj/++IOJEyfSrVs3KxJnjPELSwQu27JlCxUrViQ8PJzp06dTsWJFSpUq5XZYxpgQYr9yuuSvv/7ilVde4aabbmLcuHEANGrUyJKAMcbvrEfgglWrVhETE8PGjRt58MEH6dy5s9shGWNCmPUI/Oztt9+mbt26afcGzJo1i8KFC7sdljEmhFki8LPbbruN7t27s2nTJu666y63wzHGGBsa8rWkpCS2bt1KrrBcQF3q1atHvXr13A7LGGPSWI/AhxYvXkxERATx8fHkklxWJM4YkyNZIvCBhIQEHnzwQe6++24KFSpErVq1qFChghWJM8bkSJYIfCApKYklS5bwyiuvsHr1avLly+d2SMYYkyWbI8gmu3fv5oMPPmDAgAFUqlSJnTt3UqBAAbfDMsaYi7IewRU6e/YsEyZMIDIykqFDh6YVibMkYIwJFCHTI/jwx10sXL83Wz/z5MmTbNm6haQjSZTo8jpVq1TlxWUJsCzhnHax8UeJKJ4/W49tjDHZJWQSwcL1e7P1C1lV2bDhZ5KTk6latSo33nhjlm0jiuenXc2S2XJcY4zJbiGTCMDzhTz30bpX9BmbN2+mcuXKhIeH831kChUrVqREiRLZFKExxvifzRF46fTp07z88svUqFGDsWPHAtCgQQNLAsaYgBdSPYLLtXLlSmJiYoiNjaVLly506dLF7ZCMMSbbWI/gIt58803q1avHsWPHWLJkCTNmzKBQoUJuh2WMMdnGEkEWzp49C0DdunXp2bMnGzdupFWrVi5HZYwx2c+GhjI4cuQITz31FHnz5mXMmDFWJM4YE/SsR5DOp59+SkREBNOnTydfvnxWJM4YExIsEQAHDx7kvvvuo0OHDhQrVoxVq1YxfPhwKxJnjAkJlgiAo0eP8tVXXzFs2DBWrVpFrVq13A7JGGP8JmTnCHbt2sXMmTN54YUXqFSpErt27bIqocaYkOTTHoGItBSRLSISJyIDMtl/tYjMdfb/KCLlfBkPeK4GGj9+PJGRkQwfPjytSJwlAWNMqPJZIhCRMGAc0AqIAB4QkYgMzWKAw6paCXgLGOGreABOnvyTxo0b89hjj1G3bl02bdpEpUqVfHlIY4zJ8XzZI7gNiFPV7ar6FzAHaJehTTtgurM8H4gWH83QeorEbeCXX35h6tSpLF26lHLlyvniUMYYE1B8OUdQEtidbn0PUCerNqqaLCJJQCHgUPpGItID6AFQpkyZywomsmQBCtaJYvCwWIoXL35Zn2GMMcEoICaLVXUSMAmgdu3al3Vx/8ttI4HI7AzLGGOCgi+HhvYCpdOtl3K2ZdpGRMKBAkCiD2MyxhiTgS8TwU9AZREpLyJXAZ2ARRnaLAK6Osv3At+o3c5rjDF+5bOhIWfMvw+wFAgD3lfVTSIyBFitqouAKcBMEYkD/sCTLIwxxviRT+cIVHUJsCTDtkHplk8BHX0ZgzHGmAuzEhPGGBPiLBEYY0yIs0RgjDEhzhKBMcaEOAm0qzVFJAHYeZlvL0yGu5ZDgJ1zaLBzDg1Xcs5lVbVIZjsCLhFcCRFZraq13Y7Dn+ycQ4Odc2jw1Tnb0JAxxoQ4SwTGGBPiQi0RTHI7ABfYOYcGO+fQ4JNzDqk5AmOMMecLtR6BMcaYDCwRGGNMiAvKRCAiLUVki4jEiciATPZfLSJznf0/ikg5F8LMVl6cc38RiRWRDSKyTETKuhFndrrYOadrd4+IqIgE/KWG3pyziNzn/Kw3iciH/o4xu3nxb7uMiHwrIuucf9+t3Ygzu4jI+yJyUEQ2ZrFfRGS08/exQURqXfFBVTWoXnhKXv8GVACuAn4GIjK06Q1McJY7AXPdjtsP59wEyOss9wqFc3ba5QOWAyuB2m7H7Yefc2VgHVDQWS/qdtx+OOdJQC9nOQLY4XbcV3jODYFawMYs9rcGPgcEuB348UqPGYw9gtuAOFXdrqp/AXOAdhnatAOmO8vzgWgRET/GmN0ues6q+q2q/umsrsTzxLhA5s3PGeBfwAjglD+D8xFvzrk7ME5VDwOo6kE/x5jdvDlnBfI7ywWAfX6ML9up6nI8z2fJSjtghnqsBK4XkSt6EHswJoKSwO5063ucbZm2UdVkIAko5JfofMObc04vBs9vFIHsoufsdJlLq+pn/gzMh7z5OVcBqojIChFZKSIt/Radb3hzzoOBh0RkD57nn/T1T2iuudT/7xcVEA+vN9lHRB4CagON3I7Fl0QkFzAKeMTlUPwtHM/wUGM8vb7lInKTqh5xMygfewCYpqpvikhdPE89jFLVs24HFiiCsUewFyidbr2Usy3TNiISjqc7meiX6HzDm3NGRO4EXgTuVtXTforNVy52zvmAKOA7EdmBZyx1UYBPGHvzc94DLFLVM6r6O7AVT2IIVN6ccwwwD0BVfwDy4CnOFqy8+v9+KYIxEfwEVBaR8iJyFZ7J4EUZ2iwCujrL9wLfqDMLE6Aues4icjMwEU8SCPRxY7jIOatqkqoWVtVyqloOz7zI3aq62p1ws4U3/7Y/xdMbQEQK4xkq2u7HGLObN+e8C4gGEJHqeBJBgl+j9K9FwMPO1UO3A0mqGn8lHxh0Q0OqmiwifYCleK44eF9VN4nIEGC1qi4CpuDpPsbhmZTp5F7EV87Lc34DuA74yJkX36Wqd7sW9BXy8pyDipfnvBRoLiKxQArwjKoGbG/Xy3N+CpgsIk/imTh+JJB/sROR2XiSeWFn3uNlIDeAqk7AMw/SGogD/gT+ccXHDOC/L2OMMdkgGIeGjDHGXAJLBMYYE+IsERhjTIizRGCMMSHOEoExxoQ4SwQmxxKRFBFZn+5V7gJtj/sxtCyJSAkRme8s10xfCVNE7r5QlVQfxFJORB701/FM4LLLR02OJSLHVfW67G7rLyLyCJ6Kp318eIxwp15WZvsaA0+r6l2+Or4JDtYjMAFDRK5znqWwVkR+EZHzqo2KSHERWe70IDaKSANne3MR+cF570cicl7SEJHvROSddO+9zdl+g4h86tR+XykiNZztjdL1VtaJSD7nt/CNzl2wQ4D7nf33i8gjIjJWRAqIyE6nHhIicq2I7BaR3CJSUUS+EJE1IvK9iFTLJM7BIjJTRFbguTGynNN2rfOq5zR9DWjgHP9JEQkTkTdE5CfnXB7Nph+NCXRu1962l72yeuG5M3a981qA5074/M6+wnjurEzt1R53/nwKeNFZDsNTc6gwnmcSXOtsfw4YlMnxvgMmO8sNcerBA2OAl53lpsB6Z3kxUN9Zvs6Jr1y69z0CjE33+WnrwEKgibN8P/Ces7wMqOws18FT/iRjnIOBNcA1znpeII+zXBnPHbfguTv13+ne1wN4yVm+GlgNlHf752wv919BV2LCBJWTqlozdUVEcgPDRaQhcBZP6d1iwP507/kJeN9p+6mqrheRRngeWLLCKa9xFfBDFsecDZ6a8CKSX0SuB+4A7nG2fyMihUQkP7ACGCUis4BPVHWPeP9Yi7l4EsC3eEqcjHd6KfX4XxkQ8HxhZ2aRqp50lnMDY0WkJp7kWSWL9zQHaojIvc56ATyJ43dvgzbByRKBCSSdgSLALap6RjxVRfOkb+B8gTcE2gDTRGQUcBj4SlUf8OIYGSfNspxEU9XXROQzPHVfVohIC7x/AM4iPEntBuAW4BvgWuBI+uR3ASfSLT8JHAD+hme4N6sYBOirqku9jNGECJsjMIGkAHDQSQJNgPOeuyyeZzEfUNXJwHt4Hvm3EqgvIpWcNteKSFa/Nd/vtLkDT1XHJOB7PEkodQL2kKoeFZGKqvqLqo7A0xPJOJ5/DM/Q1HlU9bjznnfwDN+kqOpR4HcR6egcS0Tkb17+vcSrp/5+FzxDYpkdfynQy+ktISJVRORaLz7fBDnrEZhAMgtYLCK/4Bnf/jWTNo2BZ0TkDHAceFhVE5wreGaLSOpQy0t4avVndEpE1uEZbvmns20wnuGmDXiqPaaWMH/CSUhngU14nvqW/pGB3wIDRGQ98Gomx5oLfOTEnKoz8K6IvOTEMAfPc3ovZDzwsYg8DHzB/3oLG4AUEfkZmIYn6ZQD1opn7CkBaH+RzzYhwC4fNcYhIt/hudwykJ9ZYMwls6EhY4wJcdYjMMaYEGc9AmOMCXGWCIwxJsRZIjDGmBBnicAYY0KcJQJjjAlx/w8haOjPp2w9uwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mod_performance:\n",
      "     fpr       tpr  thresholds\n",
      "0   0.0  0.000000    2.000000\n",
      "1   0.0  0.071429    1.000000\n",
      "2   0.1  0.071429    1.000000\n",
      "3   0.1  0.214286    0.999975\n",
      "4   0.2  0.214286    0.999730\n",
      "5   0.2  0.714286    0.629649\n",
      "6   0.3  0.714286    0.466056\n",
      "7   0.3  0.785714    0.417499\n",
      "8   0.6  0.785714    0.106709\n",
      "9   0.6  0.857143    0.088576\n",
      "10  0.7  0.857143    0.064524\n",
      "11  0.7  0.928571    0.064519\n",
      "12  1.0  0.928571    0.013826\n",
      "13  1.0  1.000000    0.005746 \n",
      "\n",
      " Confusion matrix: \n",
      " [[ 8  4]\n",
      " [ 2 10]] \n",
      "\n",
      " Recall is: 0.83 Precision is: 0.71 Accuracy is: 0.75\n"
     ]
    }
   ],
   "source": [
    "#model validation on test data -\n",
    "#Image preprocessing on test images\n",
    "print(\"\\nLength of Test Images:\\n\", len(test_pos_imgs) + len(test_neg_imgs))\n",
    "\n",
    "test_y_1 = [] \n",
    "for i in test_pos_imgs:\n",
    "    test_y_1.append(1)\n",
    "\n",
    "  ###\n",
    "\n",
    "test_y_0 = [] \n",
    "for k in test_neg_imgs:\n",
    "    test_y_0.append(0)\n",
    "    \n",
    "test_imgs = test_pos_imgs + test_neg_imgs\n",
    "test_labels = test_y_1 + test_y_0\n",
    "\n",
    "def transform_test_imgs(filename):\n",
    "    \n",
    "    if filename in test_pos_imgs:\n",
    "        test_img = cv2.imread(test_pos + '/' + filename) #test_pos_imgs[0]\n",
    "        #test_img1 = cv2.GaussianBlur(test_img, (11, 11), 0)  # gauss_blur\n",
    "        test_img2 = cv2.resize(test_img , (150,150))   #resize\n",
    "        \n",
    "    else:\n",
    "        test_img = cv2.imread(test_neg + '/' + filename) #test_pos_imgs[0]\n",
    "        #test_img1 = cv2.GaussianBlur(test_img, (11, 11), 0)  # gauss_blur\n",
    "        test_img2 = cv2.resize(test_img , (150,150))   #resize\n",
    "    \n",
    "    return test_img2\n",
    "\n",
    "\n",
    "test_img_list = []\n",
    "for f in test_imgs:\n",
    "    test_X = transform_test_imgs(f)  #images in test_x is in same order as we have test_labels.\n",
    "    test_img_list.append(test_X)\n",
    "    \n",
    "    \n",
    "test_arr = np.array(test_img_list)\n",
    "test_arr1 = test_arr.astype('float32')\n",
    "test_arr2 = test_arr1 / 255.\n",
    "\n",
    "\n",
    "#check precision, recall, accuracy on test images\n",
    "len(test_arr2)\n",
    "predictedlabels = model1.predict_classes(test_arr2)\n",
    "predicted_probs = model1.predict(test_arr2)\n",
    "\n",
    "conf_mat = confusion_matrix(predictedlabels, test_labels)  \n",
    "Recall = conf_mat[1,1]/(conf_mat[1,1]+conf_mat[1,0])\n",
    "Precision = conf_mat[1,1]/(conf_mat[1,1]+conf_mat[0,1])\n",
    "Accuracy = (conf_mat[1,1]+conf_mat[0,0])/(conf_mat[0,0]+conf_mat[0,1]+conf_mat[1,0]+conf_mat[1,1])\n",
    "\n",
    "# RoC\n",
    "y_pred = model1.predict(test_arr2).ravel()  # flatten the array of predicted values.\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, y_pred)\n",
    "auc_keras = auc(fpr, tpr)  # 90.28 \n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "perf_metrics= list(zip(fpr, tpr, thresholds)) \n",
    "mod_perf = pd.DataFrame(perf_metrics, columns = ['fpr', 'tpr', 'thresholds'])\n",
    "print(\"\\nmod_performance:\\n {} \\n\\n Confusion matrix: \\n {} \\n\\n Recall is: {:.2f} Precision is: {:.2f} Accuracy is: {:.2f}\"\n",
    "      .format(mod_perf, conf_mat, Recall, Precision, Accuracy))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db646d6b",
   "metadata": {},
   "source": [
    "Accuracy in the final epoch on the train set=0.96\n",
    "Accuracy on the test set=0.75.\n",
    "\n",
    "This difference can be attributed to the imbalanced dataset as well as the size of the dataset. However, it can be improved in many ways. One way is to develop an algorithm to segment positive and negative glucoma images from the repository. Once we get the huge labelled data and train the model on it, the difference will certainly shrink. Additionally, the tuning of the filter parameters definitely improves the model's performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859fe1a0",
   "metadata": {},
   "source": [
    "#  Test Data: Step-by-Step Validation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "001425d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of predicted class for pos:\n",
      " 0.7142857142857143\n",
      "Proportion of predicted class for neg:\n",
      " 0.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#class label 1\n",
    "def transform_test_pos_img(filename):\n",
    "    test_img = cv2.imread(test_pos + '/' + filename) #test_pos_imgs[0]\n",
    "    test_img1 = cv2.GaussianBlur(test_img, (11, 11), 0)  # gauss_blur\n",
    "    test_img2 = cv2.resize(test_img1 , (150,150))   #resize\n",
    "    \n",
    "    return test_img2\n",
    "\n",
    "test_Xpos_list = []\n",
    "for f in test_pos_imgs:\n",
    "    test_X = transform_test_pos_img(f)\n",
    "    test_Xpos_list.append(test_X)\n",
    "    \n",
    "test_imgs_arr = np.array(test_Xpos_list)\n",
    "test_imgs_arr1 = test_imgs_arr.astype('float32')\n",
    "test_pos_img_arr2 = test_imgs_arr1 / 255.\n",
    "\n",
    "predictedclass_pos = model1.predict(test_pos_img_arr2)\n",
    "\n",
    "#len(predictedclass_pos) = 3803 which is the total number of positive test samples.\n",
    "print(\"Proportion of predicted class for pos:\\n\", \n",
    "      len(predictedclass_pos[np.where(predictedclass_pos > 0.5)])/len(predictedclass_pos) )\n",
    "\n",
    "\n",
    "    \n",
    "#class label 0\n",
    "def transform_test_neg_img(filename):\n",
    "    test_img = cv2.imread(test_neg + '/' + filename) #test_pos_imgs[0]\n",
    "    test_img1 = cv2.GaussianBlur(test_img, (11, 11), 0)  # gauss_blur\n",
    "    test_img2 = cv2.resize(test_img1 , (150,150))   #resize\n",
    "    \n",
    "    return test_img2\n",
    "    \n",
    "test_Xneg_list = []\n",
    "for f in test_neg_imgs:\n",
    "    test_X = transform_test_neg_img(f)\n",
    "    test_Xneg_list.append(test_X)\n",
    "\n",
    "\n",
    "test_imgs_arr = np.array(test_Xneg_list)\n",
    "test_imgs_arr1 = test_imgs_arr.astype('float32')\n",
    "test_negimgs_arr2 = test_imgs_arr1 / 255.\n",
    "\n",
    "\n",
    "\n",
    "predictedclass_neg = model1.predict(test_negimgs_arr2)\n",
    "\n",
    "print(\"Proportion of predicted class for neg:\\n\"\n",
    "    ,len(predictedclass_neg[np.where(predictedclass_neg > 0.5)])/len(predictedclass_neg))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2d2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##Convolution neural networks-\n",
    "#https://arxiv.org/ftp/arxiv/papers/1506/1506.01195.pdf\n",
    "#https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "\n",
    "# ImageDataGenerator accepts the original data, randomly transforms it, and returns only the new, transformed data.\n",
    "#Input data is slightly modified versions of the original input data, the network is able to learn more robust features.\n",
    "\n",
    "\n",
    "##Data Augmentation-\n",
    "#https://arxiv.org/abs/2004.13529  ; augmentation behavioural cloning to generate data.\n",
    "#file:///C:/Users/AG89382/Downloads/data-06-00014.pdf\n",
    "\n",
    "##Input Dataset-\n",
    "#https://ieee-dataport.org/documents/1450-fundus-images-899-glaucoma-data-and-551-normal-data\n",
    "\n",
    "##Other referred articles-\n",
    "#https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0263-7\n",
    "#https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
